# Global default settings.
global {
  scrape_interval: "15s"     # By default, scrape targets every 15 seconds.
  evaluation_interval: "15s" # By default, evaluate rules every 15 seconds.

  # Attach these extra labels to all timeseries collected by this Prometheus instance.
  labels: {
    label: {
      name: "monitor"
      value: "codelab-monitor"
    }
  }

  # Load and evaluate rules in this file every 'evaluation_interval' seconds. This field may be repeated.
  #rule_file: "prometheus.rules"
}

# A job definition containing exactly one endpoint to scrape: Here it's prometheus itself.
job: {
  # The job name is added as a label `job={job-name}` to any timeseries scraped from this job.
  name: "prometheus"
  # Override the global default and scrape targets from this job every 5 seconds.
  scrape_interval: "5s"

  # Let's define a group of targets to scrape for this job. In this case, only one.
  target_group: {
    # These endpoints are scraped via HTTP.
    target: "http://localhost:9090/metrics"
  }
}

## Monitor a set of a API servers.
#job {
#  # This job will be named "api-server", so a job="api-server" label will be
#  # added to all time series scraped from it.
#  name: "api-server"
#  # Discover targets for this job via service discovery (DNS-SD). The sd_name
#  # provided here needs to resolve to a DNS SRV record containing a set of
#  # IP:PORT pairs.
#  sd_name: "telemetry.server.prod.api.srv.my-domain.org"
#  # The SRV records do not have information about the endpoint to scrape, so it
#  # needs to be configured separately when discovering targets dynamically.
#  metrics_path: "/metrics"
#}
